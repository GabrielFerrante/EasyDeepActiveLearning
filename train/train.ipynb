{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8feb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from ..utils.data_utils import get_svhn_loaders, SVHNCustomDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels, _ in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Como labels são sequências (batch, max_len), \n",
    "        # a Loss depende de como você estruturou a saída do seu modelo\n",
    "        loss = criterion(outputs.view(-1, 11), labels.view(-1)) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "def validate_accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct_sequences = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Pegamos o dígito mais provável para cada uma das 5 posições\n",
    "            preds = torch.argmax(outputs, dim=2) # [Batch, 5]\n",
    "            \n",
    "            # Uma predição só é correta se TODOS os dígitos da sequência baterem\n",
    "            # (Ignorando os pads se necessário, mas aqui comparamos a sequência cheia)\n",
    "            correct_batch = torch.all(preds == labels, dim=1).sum().item()\n",
    "            correct_sequences += correct_batch\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    return 100 * correct_sequences / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_active_learning_cycle(model, train_dataset, labeled_indices, test_loader, device, config, cycle):\n",
    "    \"\"\"\n",
    "    Executa o treinamento para um conjunto específico de índices rotulados.\n",
    "    \"\"\"\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=f\"runs/AL_Cycle_{cycle}\")\n",
    "    \n",
    "    # Criar DataLoader apenas com as amostras selecionadas (Labeled Set)\n",
    "    train_subset = Subset(train_dataset, labeled_indices)\n",
    "    train_loader = DataLoader(train_subset, batch_size=config['batch_size'], \n",
    "                              shuffle=True, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=10) # 10 é o pad_token\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        test_acc = validate_accuracy(model, test_loader, device)\n",
    "        print(f\"Epoch {epoch+1}: Loss {loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "        # Dentro do loop de épocas:\n",
    "        writer.add_scalar('Loss/train', loss, epoch)\n",
    "        writer.add_scalar('Accuracy/test', test_acc, epoch)\n",
    "    \n",
    "    writer.close() \n",
    "    return model, test_acc, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2da71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETROS\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = {\n",
    "    'batch_size': 64,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 20,\n",
    "    'initial_budget': 1000, # Quantas imagens começam rotuladas\n",
    "    'cycle_budget': 500     # Quantas imagens adicionar por ciclo de AL\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((640,640)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.439, 0.434, 0.439], std=[0.204, 0.208, 0.208])\n",
    "    ])\n",
    "\n",
    "# Carregar Dataset Completo (Pool)\n",
    "# Usamos o Dataset puro aqui para podermos manipular os índices via Subset\n",
    "train_ds = SVHNCustomDataset('C:/Repositorios/EasyDeepActiveLearning/csv_SVHN/train.csv', 'F:/SVHN/train/train/', transform=transform) # Add transforms here\n",
    "test_ds = SVHNCustomDataset('C:/Repositorios/EasyDeepActiveLearning/csv_SVHN/test.csv', 'F:/SVHN/test/test/', transform=transform)\n",
    "test_loader = DataLoader(test_ds, batch_size=config['batch_size'], shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização do Active Learning\n",
    "num_train = len(train_ds)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Índices das imagens que o modelo \"pode ver\"\n",
    "labeled_indices = indices[:config['initial_budget']]\n",
    "# Índices do pool não rotulado (Unlabeled pool)\n",
    "unlabeled_indices = indices[config['initial_budget']:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_al_checkpoint(model, optimizer, cycle, labeled_indices, acc, path=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Salva o estado do modelo, otimizador e metadados do ciclo de AL.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    checkpoint = {\n",
    "        'cycle': cycle,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'labeled_indices': labeled_indices, # Essencial para saber o que foi usado no treino\n",
    "        'test_acc': acc\n",
    "    }\n",
    "    \n",
    "    filename = os.path.join(path, f\"model_cycle_{cycle}_acc_{acc:.2f}.pt\")\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint salvo: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a99973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37ede9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ..model.models import SVHNCustomCNN\n",
    "from ..query_strategies.strategies import entropy_query_strategy\n",
    "\n",
    "# Loop de Ciclos de Active Learning\n",
    "num_cycles = 5\n",
    "checkpoint_dir = \"al_checkpoints_svhn\"\n",
    "for cycle in range(num_cycles):\n",
    "    print(f\"\\n--- Iniciando Ciclo de AL {cycle} | Labeled Size: {len(labeled_indices)} ---\")\n",
    "    \n",
    "    \n",
    "    # Inicializa/Reseta o modelo para cada ciclo (ou continua o treino)\n",
    "    model = SVHNCustomCNN().to(device)\n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    # Treina o modelo com o que temos rotulado até agora\n",
    "    model, last_acc, opt = run_active_learning_cycle(model, train_ds, labeled_indices, test_loader, device, config, cycle)\n",
    "    \n",
    "    save_al_checkpoint(\n",
    "        model=model, \n",
    "        optimizer=opt, \n",
    "        cycle=cycle, \n",
    "        labeled_indices=labeled_indices, \n",
    "        acc=last_acc,\n",
    "        path=checkpoint_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"--- Iniciando Query Strategy: Entropy Sampling ---\")\n",
    "    \n",
    "    # Chamada da estratégia\n",
    "    new_indices = entropy_query_strategy(\n",
    "        model=model,\n",
    "        dataset=train_ds,\n",
    "        unlabeled_indices=unlabeled_indices,\n",
    "        budget=config['cycle_budget'],\n",
    "        batch_size=config['batch_size'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Atualiza os conjuntos para o próximo ciclo\n",
    "    # Removendo os selecionados do unlabeled_indices\n",
    "    # (Transformamos em set para uma busca mais rápida e depois voltamos para lista/array)\n",
    "    new_indices_set = set(new_indices)\n",
    "    unlabeled_indices = np.array([i for i in unlabeled_indices if i not in new_indices_set])\n",
    "    \n",
    "    # Adiciona ao labeled\n",
    "    labeled_indices = np.concatenate([labeled_indices, new_indices])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
